% SPDX-License-Identifier: Apache-2.0
% Copyright (c) Contributors to the PhysLight Project.

\chapter{Imaging and lighting models}\label{ch:lighting}
\section{Describing imaging}\label{ch:imaging}

Imaging is the process of capturing the \gls{radiant power} resulting 
from a specific \gls{scene} configuration and turning it into pixel values. 
This is done by creating a camera and sensor or film model and computing 
their response to the energy flowing through the scene.
The imaging parameters describe how specifically this conversion should happen 
are collected here together with their symbols and dimensions:

\vskip 2mm
\begin{center}
\begin{tabular}{r c l}
	focal length        & $f$ & $[\unit{\milli\meter}]$ \\
	aperture number     & $N$ & $[\unit{\fnumber}]$ \\
	focus distance      & $o$ & $[\unit{\meter}]$ \\
	exposure time       & $\Delta t$ & $[\unit{\second}]$ \\
	film speed          & $S$ & $[\unit{\iso}]$ \\
	\glsname{CCT} of the white point & $T_{cp}$ & $[\unit{\kelvin}]$ \\
	filmback dimensions &     & $[\unit{\square\meter}]$ \\
	image resolution    &     & $[\unit{\pixel}]$
\end{tabular}
\end{center}

\vskip 2mm

In our imaging model, the pixels in the image are triples of
floating point numbers, known as \textsl{\gls{tristimulus values}}, their
magnitudes being linear in the spectral radiant \gls{exposure} of the various
pixels. 
The \textsl{\gls{imaging equation}}\footnote{
	This equation is present in~\cite[Eq. 1]{kolb95} in an effectively equivalent form,
	there it's called the \emph{measurement equation}. 
	However several other authors don't include in their definition of the
	\emph{measurement equation} the aspect of integration in the time domain, which is
	of key importance for our work~\cite[p. 45, Eq 2.33]{dutre2003};~\cite{pharr2023}, 
	so to minimize confusion we chose to use a different name. 
}, relating the final color $C_{col}$ of a pixel
$p^{img}$ to the incoming \gls{spectral} \gls{radiance} may appear daunting at first
\begin{equation}\label{eqn:imaging1}
	C_{col}(p^{img})  = \frac1{A_p^{img}}
	\int_{\Delta t} \int_{\lambda} \int_{\Omega^{img}_a} \int_{A_p^{img}}
	  W(x, \lambda) L^{\downarrow
		img}_{e,\lambda}(x, \omega, \lambda, t) \cos\theta\d x\d\omega\d\lambda\d t
\end{equation}

\todo{The previous footnote: the Pharr thing is incorrect, see chapter 5.4, they do have the time dimension in there}

In order to understand what's happening, we'll start making a simplifying assumption:
we will ignore for the time being the dependency of $W(x,\lambda)$ on $x$, and 
pretend it was simply $W(\lambda)$. 
Once we have a clearer vision of how the imaging equation would work in this
simpler case, it'll be clear what to do in the more general case. 
We will get into the details of what's the meaning and purpose of $W(x,\lambda)$ 
in~\cref{sec:filmrespfunc}, for the moment we just know we need to do 
\emph{something} to the incident \gls{spectral} \gls{radiance} $L^{\downarrow}_{e,\lambda}$ to make
pictures out of it.

We've seen in~\cref{sec:lightbrightness} what's the reasoning behind a need for the values 
of our pixels to be proportional to the \textsl{\glsdisp{photometry}{luminous} \gls{exposure}} there. 
In practice two realities will manifest: one is that the pixel has a certain, 
non-$0$ area: we used the symbol $A^{img}_p$ for this little region\footnote{
	We'll be using the $\square^{img}$ superscript to indicate \textsl{\gls{image}} space 
	locations, positions on the plane where the image is formed (we call this 
	the \gls{filmback}). And we're using a $\square_p$ subscript to signify we're 
	interested in the specific location $p$, which will normally be an argument of a 
	function being defined. To reduce clutter we won't repeat the $\square^{img}$ 
	superscript in more than one place in a single symbol}. 
The other is that over this area the incoming \gls{spectral} 
\gls{radiance} won't necessarily be uniform (at least not in the general case: 
there might be a shadow boundary across the pixel, or more likely some kind of 
gradient across it). 
The innermost integral
\begin{displaymath}
	\bar L^{\downarrow img}_{e,\lambda}(p^{img}, \omega, \lambda, t) = \frac1{A_p^{img}} \int_{A_p^{img}} L^{\downarrow img}_{e,\lambda}(x, \omega, \lambda, t) \d x
\end{displaymath}
is how we replace the incident \gls{spectral} \gls{radiance} $L^{\downarrow}$ with 
its own average $\bar L^{\downarrow}$ over $A^{img}_p$, which is our pixel region located at $p^{img}$. 
There is no need for $W(\lambda)$ here: it doesn't depend on $x$ so we can factor it 
out of this integration step.

The next integral is then 
\begin{displaymath}
	E_{e,\lambda}(p^{img},\lambda,t) = \int_{\Omega^{img}_a} \bar L^{\downarrow img}_{e,\lambda}(p^{img}, \omega, \lambda, t) \cos\theta \d\omega
\end{displaymath}
which is an expression to compute \gls{spectral} \gls{irradiance} over this given
solid angle $\Omega^{img}_a$: much like $p^{img}$ is our location on the image, 
$a$ is the opening that makes up our aperture, so that 
$\Omega^{img}_a$ is the solid angle with apex at our image location $p^{img}$ (ie. on the filmback) 
that spans the aperture\footnote{
	Our eagle-eyed readers will have observed that $\Omega^{img}_a$ will have a dependency 
	on $p^{img}$ as well: we've omitted it from the symbol to avoid making the notation even 
	more complex. Note that $\cos\theta$ does take the dependency into account correctly}.
And there is no $W(\lambda)$ here either: like before it doesn't depend on $\omega$ so 
we can factor it out of this integration step too.

The next layer of the integral is where we take care of the \gls{spectral} side of things:
\begin{displaymath}
	E_{col}(p^{img}, t) = \int_{\lambda} W(\lambda) E_{e,\lambda}(p^{img},\lambda,t) \d\lambda
\end{displaymath}
and here we have to include $W(\lambda)$, obtaining something \emph{similar} to \gls{irradiance} or
\gls{illuminance}, but not quite. Rather, it's some kind of weighted average of the spectral
irradiance, where $W(\lambda)$ provides the (spectral) weighting. 
There is light at the end of the tunnel, after all (if you pardon the pun). 

The subscript $\square_{col}$ reminds us there is some weighting that has happened here, 
and of course it suggests there will have to be some relation to \emph{color} in a way or the 
other.
We can now finish turning our \gls{irradiance}/\gls{illuminance}-like quantity into 
a corresponding \gls{exposure}-like quantity integrating over time:
\begin{displaymath}
	H_{col}(p^{img}) = \int_t E_{col}(p^{img},t) \d t
\end{displaymath}
and this concludes our analysis of the \textsl{imaging equation}. 
We've reduced it to a rather innocent looking expression
\begin{displaymath}
 	C_{col}(p^{img}) = H_{col}(p^{img})
\end{displaymath}
or in words: ``to make a picture, set the color of each pixel to the exposure there'', 
but we'll want to pay attention to just what exactly we mean by ``exposure'': 
all we know so far is that using a standard kind of exposure, say radiant exposure, or
luminous exposure, won't help us in making pictures that are a good match for images
shot with a camera loaded with film stock you'd normally buy.

\subsection{The film response function}\label{sec:filmrespfunc}

With the mathematical meaning of the \textsl{imaging equation} under our belt, 
we still need to double back and understand what is the story with that original $W(x,\lambda)$ term. 
We call this function \textsl{\gls{film response function}} 
and we use it to capture how the film or sensor responds to incident \gls{spectral} \gls{radiance}
(being a whole bunch of photons landing on it). 
Because in certain situations the sensitivity is not spatially uniform across the \gls{filmback}, 
we need to keep this weighting factor right in the heart of our integral.

It will probably help gain some intuition for the purpose and meaning 
of $W$ if we go and see what happens in a few practical cases.
In the case in which $W(x, \lambda)$ is identically equal to $1$, the
result of the integral would be measured in $[\unit{\joule\per\square\meter}]$,
and we would have $H_{col} = H_e$, or in words our images would be measuring the 
\textsl{radiant \gls{exposure}} corresponding to each scene location in the image. 
As discussed in~\cref{sec:lightbrightness}, images constructed this way, even 
if they were possible\footnote{
	As discussed before, film stock or a digital device that behaves like this across a wide region
	of the electromagnetic spectrum simply cannot be built: if nothing else because photons
	larger than a pixel on the sensor will just ``bounce'' right off, and photons much smaller
	than the sensor's components will simply traverse it and continue mostly undisturbed.
	This makes it impossible to have values even roughly representative of $H_e$}  
just won't look natural, and are quite unlikely to have much practical use at all.

That same discussion might inspire us to use $W(x,\lambda) = K_{cd} V(\lambda)$,
which would result in $H_{col} = H_v$: doing this would give us images where the
pixel values are a direct readout of the scene's \textsl{luminous \gls{exposure}}.
We'd be doing much better with a choice like this: now we would have images that track 
fairly well a human's sense of brightness of a scene, 
and provide a passable, but not very accurate, approximation of
an orthochromatic film stock with reduced blue sensitivity\footnote{Silver halides
	are very sensitive to short-wavelength light, so film stock with reduced
	sensitivity in the blues is exceptionally difficult to produce with usual
	photosensitive chemicals}, 
the difference from the look of what we'd normally consider to be a black-and-white
photo is illustrated in~\cref{fig:orthopanphoto}.

The reality is that in practice the film response functions 
that one finds in common use are typically modeled as a product of
components, each dependent either on wavelength or on position, 
and so we decompose it as follows
\begin{equation}
W(x,\lambda) = k_i S W_{pos}(x) W_{col}(\lambda)
\end{equation}
where $k_i$ is the \textsl{imaging constant} (defined in~\cref{eqn:imaging_ki}), 
$S$ is the \gls{film speed},
$W_{pos}(p^{img})$ describes the \textsl{local response} of the filmback 
and
$W_{col}(\lambda)$ describes its \textsl{spectral response}.

\subsubsection{Spectral response}
We've already seen a couple examples of candidates for the spectral component
of the \gls{film response function}. These however had mostly theoretical value,
to illustrate the purpose of the film response function as a whole.
In order for us to make ordinary color images, we need to produce data
suitable to convert into a color space of our liking.

Color spaces are normally defined using a so-called $3\times3$ matrix, which
transforms data from \gls{CIE} \gls{XYZ} coordinates to the space of interest. 
For example to obtain data appropriate for the \gls{sRGB} primaries, starting from
a color $C_{\XYZ}$ in \gls{CIE} \gls{XYZ} coordinates stored in a column vector\footnote{
	For what concerns geometric coordinates and in general the \emph{spatial} side of things
	in graphics, there seems to be a relatively even split in literature between authors that prefer 
	to represent positions stored in \textsl{row vectors} (1-row matrices) 
	or \textsl{column vectors} (1-column matrices).
	However when it comes to color it's quite clear that the overwhelming majority of the authors
	store color triples as \textsl{column vectors}. 
	We hereby take a moment to signify our gratitude for this state of things
}, we would use
\begin{displaymath}
	C_{\sRGBl} = M_{\sRGBl} C_{\XYZ} = \left[\begin{array}{r@{.}lr@{.}lr@{.}l}
		3&2404542 & -1&5371385 & -0&4985314 \\
		-0&9692660 &  1&8760108 &  0&0415560 \\
		0&0556434 & -0&2040259 &  1&0572252 \\
	\end{array}\right] C_{\XYZ}
\end{displaymath}

The reason for this is that the \gls{CIE} \gls{XYZ} color space, described in~\cite{cie:015.2018},
is a space that was built\footnote{
	The \gls{CIE} \gls{XYZ} color space was very carefully derived from measurements
	done in the late 1920's by W. David Wright and John Guild to capture 
	as accurately as possible how humans see color. 
	The color space was introduced in~\cite{smithguild1931} and 
	a fascinating account of the process was collected in~\cite{fairman97} 
} to capture the response to color of the human visual system. 
This makes it effectively a \emph{lingua franca} of color, that people can
use to objectively capture and reproduce a color sensation.

To state in our notation what's commonly done in digital image synthesis 
we would construct a film response function $W_{\XYZ}(\lambda)$ defined 
like this:
\begin{equation}
	W_{\XYZ}(\lambda) = 
	\left(
	  \begin{array}{c}
	  	  \bar x(\lambda)\\ 
	  	  \bar y(\lambda)\\ 
	  	  \bar z(\lambda)
	  \end{array}
	\right)
\end{equation}
this is a function that given a wavelength $\lambda$ returns a column vector
made from the values of the three defining functions of \gls{CIE} \gls{XYZ} space,
namely $\bar x(\lambda), \bar y(\lambda), \bar z(\lambda)$.

This is a notational convenience that saves us the work to write down the
imaging equation three times: with $W_{\XYZ}(\lambda)$ defined this way,
we obtain a result $C_{\XYZ}$ in \gls{CIE} \gls{XYZ} space.
We can then construct our \gls{sRGB} image multiplying this result by
the $M_{\sRGBl}$ matrix as above and applying a gamma function as needed
according to the definition of \gls{sRGB} space (details on this and other 
color space transformations are available in~\cref{ch:implementation}).

Equivalently, we can simply transform the \gls{CIE} \gls{XYZ} function
samples for each wavelength $\lambda$ of interest, thus obtaining a new 
response function that construct for us directly linear \gls{sRGB} values:
\begin{displaymath}
 W_{\sRGBl}(\lambda) = M_{\sRGBl} W_{\XYZ}(\lambda)
\end{displaymath}

Proceeding this way one would obtain pictures with colors that a human observer,
or rather our mythical friend the \emph{standard observer}, could not
distinguish from the sensation they would have had if they had been standing 
in the scene at the moment when the image was taken.

\paragraph{Camera sensitivity functions}
Although using \gls{CIE} \gls{XYZ} spectral response is common practice
in digital image synthesis (see for example \cite{pharr2023, jakob2022mitsuba3,
ward1994}), we actually have in front of us a different, intriguing opportunity
to explore. 
In fact it's quite evident that this is not how a real-life camera actually works,
be that digital or film-based. 
Rather, the sensing component in the device will have its own spectral response,
defining a camera-specific \gls{RGB} space which we'll call \camRGBl.
For analogy with how \gls{CIE} \gls{XYZ} space is defined, we'll call these
response curves $\bar r(\lambda), \bar g(\lambda), \bar b(\lambda)$, where the 
$r,g,b$ function names indicate these correspond to our red, green and blue photosites, and
the overbar indicates that some normalization had been applied (a common approach is for the 
three curves to be scaled so their collective peak value, usually occuring in $\bar g$, is $1$).
With this in hand, our spectral film response function would be
\begin{equation}
	W_{\camRGBl}(\lambda) = 
	\left(
	\begin{array}{c}
		\bar r_{\camRGBl}(\lambda) \\
		\bar g_{\camRGBl}(\lambda) \\
		\bar b_{\camRGBl}(\lambda)
	\end{array}
	\right)
\end{equation}
giving us a framework which will make our rendered images match our ``camera raw'' 
data.

This is a valuable thing to do in the interest of matching the metamerism of the 
camera sensing element and in general its specific ability to perceive color, 
which defines its ``look'' from a chromatic perspective. 
In turn this will eliminate one source of discrepancies in the appearance of 
photographed objects when compared to renders of their virtual counterparts.

At this point, the same image processing pipeline used for the real camera
that is being modeled will apply unchanged to the rendered data, further 
details are outlined in~\cref{ch:implementation}.

\begin{inconstruction}
	Discuss exposure/density response curves here
	(or cross-reference it from elsewhere)
\end{inconstruction}


\subsubsection{Local response}
Let's look for a moment at a pixel that is away from the center of the image.
Let's imagine that this pixel is situated so that the ray going through its center
and the center of the iris forms an angle $\theta$ with the axis of the lens.
There is in all photographs a darkening that is usually radial happening from the
center of the image and increasing towards the corner, colloquially known as ``vignetting''.

To talk more simply about this effect we will introduce the notion of \textsl{\gls{paraxial ray}},
being intuitively a ray that is nearly parallel to the main axis of the lens system and running 
close to it, so in an everyday camera, this would be a ray (or rather a path) coming from an 
object very sitting near the center in image space. 
If an object sits away from the center in image space, there will be ray paths that travel to the 
image plane and do so in a manner that intersects the lens axis (called \textsl{meridional rays}, 
these paths will entirely lie in the plane containing the lens axis, the object and its image),
and ray paths that don't do so (called \textsl{skew rays})\footnote{ 
	The analysis of the behaviour of these three classes of paths is of course a key element of 
	the design process of lens systems, and is covered in depth in books such 	
	as~\cite{kingslake2010,smith2008}. We direct readers interested in a somewhat more
	casual treatment of the subject towards the excellent~\cite{kingslake92}}.
We will refer to meridional and skew rays collectively as \textsl{\glspl{oblique ray}}.

Vignetting happens for three broad classes of reasons: 
	the first one is that the geometry of the problem induces a certain attenuation onto \glspl{oblique ray}; 
	the second is that the lens system housing physically impedes certain \glspl{oblique ray} from 
	reaching into the lens system (so it's a form of shadowing);
	the third one is that the lenses distort the apparent image of the \gls{iris} as seen by the
	rays (which in some designs can and does send \emph{more} light to image locations away from the
	image center).

In many renderers there no lens being modeled and the imaging process happens as if the
lens \gls{iris} is point-sized: this is called the \textsl{\gls{pinhole camera} model}.
Note that in a pinhole camera, the point-sized iris is still thought of a lying on a known plane,
which gives us well-defined notion of lens axis, and lets us reason about the angle between the
image plane and the lens system, even when in fact there are no lenses.
In a pinhole camera all oblique rays are meridional (because the point-sized iris forces
them to go through the lens axis), there is no shadowing by the lens system housing (as there
is no provision in the model for it), and there are no distortions or aberrations (also because
the is no modeling of a lens system).

\todo{Resolve the $\theta$ vs $\theta_x$ thing}

However, even in a pinhole camera the geometric aspects of what we've called vignetting remain, 
and they further decompose in three phenomena:
\begin{itemize}
	\item as the radiance traverses the iris under an angle $\theta$,
	      the iris's apparent size gets reduced by a factor of $\cos\theta$ (remember how we
	      want to compute flux thinking of the aperture shape being orthogonal
	      to the ray)
	\item as the radiance arrives at the filmback also under an angle $\theta$
		  (the filmback is orthogonal to the lens axis) it spreads over
		  a larger area, decreasing its area density by a second factor $\cos\theta$
	\item as the radiance travels \emph{obliquely} between the iris and the pixel,
		  the distance it covers grows by a factor $1/\cos\theta$ (also if the image plane is
		  orthogonal to the lens axis) thereby the area density
		  at the pixel is further attenuated by a factor of $\cos^2\theta$ (this is
		  the inverse-square law)
\end{itemize}
the compound effect of these three phenomena gives us the so-called ``$\cos^4$ law''\footnote{
	This explanation follows closely the one in~\cite[Ch. 6]{kingslake92}, but there are 
	similar derivations in many quality books on physically based rendering such 
	as~\cite[Ch. 5.4]{pharr2023}, and obviously in treatments of lens design}, 
stating that the irradiance (or exposure) at a pixel is reduced by a factor $\cos^4\theta$.

It is usually undesirable to model the effect of the $\cos^4$ law in a \gls{renderer},
because most workflows that use images would end up removing it with a postprocess anyways\footnote{
Vignetting is sometimes reintroduced for artistic effect, yet another case where a ``defect'' is 
repurposed into a creative opportunity, see discussion on page \pageref{defects_as_opportunity}},
so it would be customary to employ a position-dependent component of the sensor 
response function equivalent to
\begin{equation}
	W_{pos}(x) = \frac{1}{\cos^4\theta_{x}}
\end{equation}
which would compensate this effect away.

When a lens system is fully modeled, the effect of the  $\cos^4$ law is usually dominated 
by what's properly called \textsl{\gls{vignetting}}, 
which is the phenomenon where the barrel housing of the lens system 
normally prevents it from admitting as much light for oblique rays as it can admit for 
paraxial rays, both because the front elements need to have a reasonable size and because of the 
numerous baffles present internal to the barrel inbetween the various elements
to reduce the propagation of stray light\footnote{
	There is an interesting observation in~\cite{kingslake92} that when working with negative
	stock that is printed on paper, usually there is passable compensation of the vignetting
	in the negative by the vignetting coming from the optics in the enlarger itself, 
	producing prints with significantly less darkening at the corners than one
	might expect}.

\todo{Provide pictures showing internal lens baffles. 
	Provide a plot of attenuation due to vignetting vs $\cos^4$.
	Show a plot of a lens with inverse vignetting (sugg Aviogon/Biogon)}



Note that we're using $x$ here and not $p^{img}$: this function is used within
our innermost integral, the one we used to compute  
$\bar L^{\downarrow img}_{\lambda}(p^{img}, \omega, \lambda, t)$
where $x$ varies over $A_p^{img}$ to compute the average of incoming
spectral \gls{radiance} across our filmback pixel area.

\todo{rephrase following and merge with previous discussion}

If a full camera lens is simulated as part of the rendering, usually the expression
for vignetting due to purely geometric considerations comes to a loss by a 
factor of $\cos^4\theta$. 
This loss will compose with light lost in various internal fixed 
diaphragms used to minimize stray light interreflections in the lens construction.
In fact in a practical lens, the effective aperture seen by a given 
location on the \gls{filmback} will be equal to what the aperture number would 
imply only for the so-called \textsl{paraxial rays}: rays that are effectively parallel to 
the lens axis (for these rays $\theta$ is effectively $0$, so the vignetting loss is also
negligible).
These ``barrell losses'' are usually significantly stronger than vignetting for non-paraxial
rays, the effect increasing with $\theta$.
This results in the common experience that wide-angle lenses have stronger vignetting
than lenses with longer focal lengths.
Digital cameras and digital photo processing software often include compensation 
tables in their firmware to compensate this effect to varying degrees.  
Dependin on the needs for accurate matching various approaches, in rendering or
postprocessing, can be employed to counteract this effect as desired.


\paragraph{Pixel filtering}
The real implementation will actually have a wider integration support,
and $W_{pos}(x)$ will normally also include a band-limiting function\footnote{
	In rendering this is sometimes called a \textsl{\gls{pixel filter}}, and it's 
	been a very active area of research. See~\cite{pharr2023} for a detailed
	introduction to the subject}, 
used to reduce or eliminate the emergence of \emph{moir\'e patterns} in the image. 
Introducing band-limiting in the image limits sharpness attainable in pictures,
which is undesirable in certain application domains. 
The evolution of digital camera products has been interesting in this respect:
the early digital cameras simply had a \gls{CCD} or \gls{CMOS} sensor at the 
filmback and then users started complaining from the appearance of moir\'e 
patterns in their pictures. 
To alleviate this, several of the manufacturers introduced cameras where a 
blurring filter was mounted in front of the sensing element, 
so that moir\'e was reduced or eliminated for most subjects. 
However, as camera resolutions became higher and higher some users became unhappy again
with these blurring filters, lamenting excessive blur in certain conditions, 
and models without this filter were reintroduced as premium niche products, 
for example \companyname{Nikon}'s \productname{D800E} model from 2014.

\todo{Add discussion of Airy disk and diffraction-limited imaging}

\paragraph{Imaging constant}

The remaining factor in the sensor response function $W(x,\lambda)$ is the
imaging constant $k_i$, which is defined as
\begin{equation}\label{eqn:imaging_ki}
	k_i = \frac{4K_{cd}}{C} = \frac{4\cdot683}{312.5} = 8.7424.
\end{equation}
The constant $C$ (valued at $312.5 = 100 \frac{25}8$ in this document) is sometimes called the \textsl{incident light meter
calibration constant} and effectively defines the units for the \gls{film speed} $S$.
Note that as we use exact integration, our value for $C$ is close to what would be used on a meter fitted with a hemispherical receptor and somewhat lower, because our exact implementation doesn't suffer from losses at near-grazing angles as a physical device would.

\begin{inconstruction}
	Discussion covering the use of $C$ versus $K$ calibration constants and
	the use of flat-receiver (cosine response) versus hemispherical receiver 
	(cardioid response).
\end{inconstruction}

\subsubsection{Gradual shutter opening}

\begin{inconstruction}
	Discuss gradual shutter opening: because of how it interacts with motion blur
	it's normally implemented before the imaging equation.
	Note also that in~\cite{kolb95} the gradual shutter opening is implemented as part of what here
	we call $W$, whereas we assume it'll be inside $L$
\end{inconstruction}

\section{Luminance and sensor response}

% todo: double check that color responsivity in XYZ or in camera RGB is an argument that comes across well

We initially posed the question of how a given pixel value concretely
relates to the measurement. As we will derive below,
Equation~\eqref{eqn:imaging1} can be simplified to the following
compact form.
\begin{equation}\label{eqn:imaging_response}
	Y(p^{img}) \approx \frac{ \pi\;\Delta t\;S}{C\; N^2} L_v^{\uparrow obj},
\end{equation}
which directly relates the luminance emitted towards the camera pupil
with the luminance of the corresponding sensor response value. This approximation
is valid for distant to mid-field objects for which angular variation
is insignificant. For simplicity we confine ourselves here to luminance, although in
reality the sensor response is carried out in \textit{camera RGB}. However, the
analysis can be carried out analogously by applying the camera
response curves instead of $\bar y$. Furthermore, due to physical
constraints and limited storage precision, physical camera sensors
obey the linearity of \cref{eqn:imaging_response} only within certain
ranges of incoming light: while most of them are very close to being linear for the
majority of their range, they will of course stop responding at the upper limit of 
brightness (they ``clip'') and often exhibit a mild over-sensitivity in the darkest 
side of the range. This behavior is very specific to each device, 
for example cameras meant for use in astrophotography contain provisions 
to better linearize the sensitivity to dim light.

\todo{Add a couple plots of responsivity curves to illustrate the point}

From this it directly becomes obvious that if the aperture number $N$,
\gls{exposure time} $\Delta t$, and \gls{film speed} $S$ are set to satisfy the
so-called \emph{exposure equation}
\begin{equation}\label{eqn:imaging_Ev}
E_v = C \frac{N^2}{\Delta t\; S} = \frac{C\; 2^{EV}}S
\end{equation}
the image of a Lambertian reflector with albedo $\rho$ (i.e.,
$L_v^{\uparrow obj} = E_v \frac \rho \pi$) at the center of the sensor
will have luminance $Y(p^{img}) = \rho$ if the illuminance incident on
the reflector is $E_v$. Or in other words, given a measured scene
illuminance, camera exposure values chosen to satisfy the exposure
equation thus yields pixel values in a reasonable range.

\begin{figure}[t]
    \centering
    \def\svgwidth{0.9\linewidth}
    \import{figures_built/}{exposure_equation_setup.pdf_tex}
    \caption{\label{fig:exposure_equation_setup}%
        Setup for our proof about the exposure equation. The Lambertian reflector with albedo $\rho$ is parallel to 
        the image plane, and the incident illumination is $E_v$. }
\end{figure}

To see why Equation~\eqref{eqn:imaging_response} is true, first consider that in the center of the sensor,
$W_{pos}(x) = 1$. The sensitivity then reduces to $W(x,\lambda) = k_i\;S\;\bar y(\lambda)$,
and we can rewrite Equation~\eqref{eqn:imaging1} in terms of luminance
$L_v^{\downarrow img}$ passing through the aperture:
\begin{equation}\label{eqn:imaging_y}
Y(p^{img}) 
           = \frac{k_i\; \Delta t \; S}{K_{cd}\;A^{img}_p} \;
             \int_{A_p^{img}} 
             \int_{\Omega_a^{img}} 
                L^{\downarrow img}_{v}(x, \omega) \;
                \cos\theta \d x \d\omega.
\end{equation}
The integration along the time dimension $t$ reduces to a multiplication
by $\Delta t$ due to the illumination field being constant in time. 

Each point $p^{img}$ on the filmback corresponds to a point $p^{obj}$ on the
focus plane in object space. Energy conservation demands that the luminous flux
$\Phi_{av}$ through the aperture must have the same value when computed from the
sensor side or the object side. Assuming the aperture is a disk parallel to 
the filmback plane, and using the luminance $L_v^{\uparrow obj}$ reflected off
the Lambertian surface into the imaging system,
\begin{equation}\label{eqn:imaging_energyconservation}
    \begin{split}
    \Phi_{av} &= \int_{A_p^{img}} \int_{\Omega_a^{img}} L_v^{\downarrow img}(x,\omega) \cos\theta \d\omega \d x \\
              &= \int_{A_p^{obj}} \int_{\Omega_a^{obj}} L_v^{\uparrow obj}(x,\omega) \cos\theta \d\omega \d x  \\
              &= A_p^{obj} \Omega_a^{obj} L_v^{\uparrow obj} 
    \end{split}
\end{equation}
The last equality assumed that $L_v^{\uparrow obj}(x,\omega)$ has
negligible variation over $(x,\omega)\in A_p^{obj}\times\Omega^{obj}_a$
and $\cos\theta \simeq 1$ for all $\omega\in\Omega^{obj}_a$. 
In other words, the aperture is small as seen from $p^{obj}$
and $p^{obj}$ is very near the optical axis of the system. 
Consequently,
\begin{equation}
    \int_{A_p^{img}} \int_{\Omega_a^{img}} L_v^{\downarrow img}(x,\omega) \cos\theta \d\omega \d x 
     = A_p^{obj} \Omega_a^{obj}L_v^{\uparrow obj}.
\end{equation}
Substituting into Equation~\eqref{eqn:imaging_y}, we obtain
\begin{equation}\label{eqn:pixel_value_area_solidangle}
  Y(p^{img}) = \frac{k_i\; \Delta t \; S}{K_{cd}\;A^{img}_p} \; A_p^{obj} \Omega_a^{obj}L_v^{\uparrow obj}.
\end{equation}

\begin{figure}[t]
    \centering
    \def\svgwidth{0.9\linewidth}
    \import{figures_built/}{a_img_obj.pdf_tex} 
    \caption{\label{fig:aperture_distance}%
        Relating image and object size in a camera where the aperture is small compared to the distance
        $o-a$ between the aperture and the object.}
\end{figure}

At this point, in order to compute $Y(p^{img})$ we need to compute the ratio of
areas $A_p^{obj}/A_p^{img}$, and the solid angle $\Omega_a^{obj}$. Let $a$ be the distance between the filmback and the
aperture on the optical axis and $o$ the distance between filmback and focus
plane. From \cref{fig:aperture_distance}, we can see that
\begin{equation*}
    \frac{A_p^{obj}}{A^{img}_p}
    = \frac{r_o^2}{r_i^2}
    = \frac{t_o^2 \sin^2\theta}{t_i^2 \sin^2\theta}
    = \frac{t_o^2}{t_i^2} \cdot 1
    = \frac{t_o^2 \cos^2\theta}{t_i^2 \cos^2\theta}
    = \frac{(o-a)^2}{a^2}.
\end{equation*}
The solid angle $\Omega_p^{obj}$ can be computed assuming that the radius of the
aperture 
\begin{equation}\label{eqn:aperture_radius}
    r = \frac{f}{2\;N}
\end{equation}
is much smaller than the distance $o-a$ between the object
and the center of the aperture. In this configuration we have simply
\begin{equation}\label{eqn:imaging_omegapobj}
\Omega_p^{obj} \simeq \frac{\pi r^2}{(o-a)^2} \qquad r \ll o-a.
\end{equation}
Substituting both results into Equation~\eqref{eqn:pixel_value_area_solidangle}, we obtain
\begin{equation}\label{eqn:imaging_y2}
Y(p^{img}) =  \frac{k_i\; \Delta t \; S}{K_{cd}\;} \frac{(o-a)^2}{a^2} \frac{\pi\;r^2}{(o-a)^2}\; L_v^{\uparrow obj}.
\end{equation}
If the object distance $o$ is far greater than the focal length $f$ we have
\begin{equation}
f \ll o \Rightarrow  f \simeq a
\end{equation}
and thus
\begin{equation}
  Y(p^{img}) =  \frac{k_i\; \Delta t \; S}{K_{cd}\;} \frac{\pi\;r^2}{a^2}\; L_v^{\uparrow obj} \stackrel{\eqref{eqn:aperture_radius}}{=}
   \frac{\pi\; k_i\; \Delta t \; S}{4\;N^2\;K_{cd}} L_v^{\uparrow obj} 
\end{equation}
which using Equation~\eqref{eqn:imaging_ki} concludes our proof. \hfill $\square$

%%
%   \begin{inconstruction}
%   
%   \paragraph{Physical units of pixel values}
%   An expression similar to the formula for $Y$ lets us determine illuminance at a pixel:
%   \begin{equation}
%   Y_v(p^{img}) = \frac{1}{A_p^{img}} \int_{A_p^{img}} \int_{\Omega_a^{img}}
%   K_{cd} \int_\lambda L_{\lambda}^{\downarrow img}(x,\omega,\lambda) \bar y (\lambda) \cos\theta \d\omega \d x \d\lambda
%   \qquad [\unit{\lux}]
%   \end{equation}
%   \end{inconstruction}

\paragraph{Applying white balance}
White balance is a color correction step intended to give a certain spectral distribution 
 neutral gray coordinates in the target color space.
The whitepoint is often specified using the \gls{CCT} of the given color,
sometimes augmented by a second value called \textsl{tint} that is intended to represent a color shift 
in some other direction in the chromaticity plane. 
    
Conceptually, the idea is that color temperature correction gives an adjustment along the orange-blue axis, 
and tint provides a mean to correct along the remaining dimension of the chromaticity space, 
which is a green-magenta shift.

Unfortunately there is little agreement in the implementations as to how exactly this should be performed:
the color correction temperature seems to be mostly understood as the color of a black body emitter for
temperatures below $4000\unit{\kelvin}$ and of \gls{CIE} Illuminant D above that. 

The case of the tint correction is somewhat more complicated with various interpretations 
of how such an axis should be oriented: in some descriptions the axis is orthogonal to the 
spectral locus of the black body emitter, so that the orientation depends on the specific 
\gls{CCT} chosen. In others, it has a constant orientation. 

Further, the amount of color shift along the tint axis has various ranges such as $[0,1]$, 
$[-100,100]$ and various interpretation as to whether the amount of apparent adjustment should be 
uniform along the range or not.

On the other hand, there seems to be consensus that white balance be implemented as a component-wise
division performed in camera \gls{RGB} coordinates, meant to divide out the coloration of the target
whitepoint from the image. In order to not introduce excessive brightness alterations
to the image, the correction color is adjusted to have either unit luminance $Y$ in \gls{CIE} \gls{XYZ} 
coordinates or unit camera \gls{RGB} green component, an approximation of unit brightness, 
useful for compute-constrained devices such as digital cameras. 

This means that the image would be computed in camera \gls{RGB} coordinates, divided by the color of 
the whitepoint in camera \gls{RGB} coordinates, and then multiplied by the camera \gls{RGB} values 
obtained by inverse transforming $(1,1,1)$ from the target \gls{RGB} space to the camera \gls{RGB} 
space.

